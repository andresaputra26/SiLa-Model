# -*- coding: utf-8 -*-
"""SiLa Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bYWcuJGLrtJRxewWWY46wHLBozSGg001
"""

import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import tensorflow as tf
import json
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

"""### Load data dari Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

# Specify the path to your folder in Google Drive
folder_path = '/content/drive/MyDrive/Dataset/Dataset SiLa/Dataset A-Z & Space'

# List all files in the folder
csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

# Create an empty dictionary to store dataframes
dataframes = {}

# Read each CSV file into a pandas DataFrame
for file in csv_files:
    file_path = os.path.join(folder_path, file)
    df_name = os.path.splitext(file)[0] # Use the filename without extension as the dataframe name
    dataframes[df_name] = pd.read_csv(file_path)

"""### Merge Data"""

def combine_all_csv(folder=folder_path):
    all_data = []
    for filename in os.listdir(folder):
        if filename.endswith(".csv"):
            df = pd.read_csv(os.path.join(folder, filename))
            all_data.append(df)
    data = pd.concat(all_data, ignore_index=True)
    return data

if __name__ == "__main__":
    data = combine_all_csv()
    print(data.head())
    print(f"Jumlah data total: {len(data)}")
    data.to_csv("dataset_all.csv", index=False)
    print("[INFO] Dataset gabungan disimpan sebagai dataset_all.csv")

"""### Data Understanding"""

df_all = pd.read_csv("dataset_all.csv")
df_all.head()

df_all.info()

"""### Mengatasi Missing Values"""

df_all.isnull().sum()

"""Tidak ada missing value

### Mengatasi data duplikat
"""

print(f'Jumlah data duplikat:', df_all.duplicated().sum())

"""Tidak ada data duplikat

### Mengecek besaran data perlabel
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'data' dataframe is already loaded and contains a 'label' column
if 'label' in data.columns:
    # Count the occurrences of each label
    label_counts = data['label'].value_counts().sort_index()

    # Create the visualization
    plt.figure(figsize=(15, 6))
    sns.barplot(x=label_counts.index, y=label_counts.values, palette='viridis')
    plt.title('Jumlah Data per Label (A-Z)')
    plt.xlabel('Label')
    plt.ylabel('Jumlah Data')
    plt.xticks(rotation=0) # Keep labels horizontal for clarity
    plt.grid(axis='y', linestyle='--')
    plt.show()
else:
    print("Kolom 'label' tidak ditemukan dalam dataframe.")

"""Pada visualisasi terlihat bahwa besaran data setiap label seimbang sebesar 200 data perlabel

### Split data dan menyimpan label
"""

# Load dataset
data = pd.read_csv("dataset_all.csv")
X = data.drop("label", axis=1).values
y = data["label"].values

# Label Encoding
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Simpan label encoder ke file JSON
label_mapping = {label: int(idx) for idx, label in enumerate(le.classes_)}
with open("label.json", "w") as f:
    json.dump(label_mapping, f)
print("[INFO] label.json disimpan.")

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

input_dim = X.shape[1]
num_classes = len(le.classes_)

"""### Modeling"""

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    epochs=50, batch_size=32,
    validation_split=0.1, verbose=1
)

loss, acc = model.evaluate(X_test, y_test)
print(f"[RESULT] Akurasi pada data uji: {acc * 100:.2f}%")

"""### Evaluasi"""

plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Training & Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Training & Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Get predictions from the model
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Get the class labels from the LabelEncoder
class_labels = le.classes_

# Plot the confusion matrix
plt.figure(figsize=(15, 12))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

"""### Save Model to H5"""

model.save("gesture_mlp_model.h5")
print("[INFO] Model disimpan sebagai gesture_mlp_model.h5")